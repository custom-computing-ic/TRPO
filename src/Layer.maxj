import com.maxeler.maxcompiler.v2.kernelcompiler.KernelLib;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVector;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVectorType;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Mem.RamWriteMode;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.Counter;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.WrapMode;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.Reductions;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.Accumulator;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.KernelMath;
import com.maxeler.maxcompiler.v2.kernelcompiler.Optimization;
import com.maxeler.maxcompiler.v2.utils.MathUtils;

import maxpower.KernelBinaryOp.Add;
import maxpower.TreeReduce;

/**
 * A Forward Propagation Layer
 *
 * @author Patrick Shao
 *
 * @param <ID> The ID of this Layer
 *             Note: Set ID to 0 for input layer, so that PrevRyLayer will be ignored as values are zero. 
 * @param <AcFunc> Activation Function of this Layer
 * @param <InBlockDim> Block Dimension of the Input Vector
 * @param <NumInBlocks> Number of Input Blocks => InputDim = InBlockDim * NumInBlocks
 * @param <OutBlockDim> Block Dimension of the Output Vector
 * @param <NumOutBlocks> Number of Output Blocks => OutputDim = OutBlockDim * NumOutBlocks
 * @param <PrevLayer> y values of previous layer, width should be NumInBlocks.
 *                    Iterating every cycle during computation, using InBlockDim cycles to finish 1 Input Vector
 * @param <PrevRyLayer> Ry values of previous layer, width should be NumInBlocks
 *                    Iterating every cycle during computation, using InBlockDim cycles to finish 1 Input Vector
 * @param <VW> V Weight, Grid Vector with NumOutBlocks items of DFEVector with NumInBlocks (GridVecType)
 *                    Iterating every cycle during computation, using InBlockDim * OutBlockDim cycles to finish
 * @param <VBias> VBias of next layer, width should be NumOutBlocks
 *                    Iterating every InBlockDim cycles during computation, using OutBlockDim iterations to finish
 * @param <beginFwdProp> begin forward propagation
 *
 */

class Layer extends KernelLib{

    private DFEVector<DFEVar> YVec;
    private DFEVector<DFEVar> RyVec;
    private DFEVar Valid;
		
    public DFEVector<DFEVar> getY()             {return YVec;}
    public DFEVector<DFEVar> getRy()            {return RyVec;}
    public DFEVar isValid()                     {return Valid;}
	
    Layer(  KernelLib owner,
            int ID,
            char AcFunc,
            int InBlockDim,
            int NumInBlocks,
            int OutBlockDim,
            int NumOutBlocks,
            DFEVector<DFEVar> PrevLayer,
            DFEVector<DFEVar> PrevRyLayer,
            DFEVar beginFwdProp
            )
    {

    super(owner);
    
    /////////////////////// Data Types ///////////////////////
    
    DFEVectorType<DFEVar>  InVecType = new DFEVectorType<DFEVar>(Def.DataType, NumInBlocks);
    DFEVectorType<DFEVar> OutVecType = new DFEVectorType<DFEVar>(Def.DataType, NumOutBlocks);
    DFEVectorType<DFEVector<DFEVar>> GridVecType = new DFEVectorType<DFEVector<DFEVar>>(InVecType, NumOutBlocks);
    DFEType AddrType = dfeUInt(MathUtils.bitsToAddress(InBlockDim*OutBlockDim));


    /////////////////////// Address Generation ///////////////////////

    // Counter Reset
    DFEVar Reset = beginFwdProp;

    // Top Level Control
    AddrGenerator CompCounter = new AddrGenerator(owner, 1, InBlockDim*OutBlockDim, Reset);
    DFEVar isCalculating = CompCounter.isValid();

    // Row Address in InBlockDim domain - to be used by Weight
    Count.Params RowAddrCounterParam = control.count.makeParams(MathUtils.bitsToAddress(InBlockDim))
                                                  .withInitValue(0)
                                                  .withMax(InBlockDim)
                                                  .withWrapMode(WrapMode.COUNT_LT_MAX_THEN_WRAP)
                                                  .withReset(Reset);
    Counter RowAddrCounter = control.count.makeCounter(RowAddrCounterParam);
    DFEVar RowAddr = RowAddrCounter.getCount();

    // Col Address in OutBlockDim domain - to be used by Weight, Bias and Temp Result
    Count.Params ColAddrCounterParam = control.count.makeParams(MathUtils.bitsToAddress(OutBlockDim))
                                                  .withInitValue(0)
                                                  .withMax(OutBlockDim)
                                                  .withEnable(RowAddrCounter.getWrap())
                                                  .withWrapMode(WrapMode.COUNT_LT_MAX_THEN_WRAP)
                                                  .withReset(Reset);
    Counter ColAddrCounter = control.count.makeCounter(ColAddrCounterParam);
    DFEVar ColAddr = ColAddrCounter.getCount();

    // Read Address - vertical in our case
    DFEVar WReadAddr = RowAddr.cast(AddrType) * OutBlockDim + ColAddr.cast(AddrType);

    // The result is valid on the last cycle of each inner loop iteration
    Valid = isCalculating & RowAddrCounter.getWrap();


    /////////////////////// Memory Allocation ///////////////////////
    
    // Weight and VWeight Memory Blocks
    DFEVector<DFEVector<DFEVar>>  WGridVec = GridVecType.newInstance(this);
    DFEVector<DFEVector<DFEVar>> VWGridVec = GridVecType.newInstance(this);
    for (int X=0; X<NumInBlocks; ++X) {
        for (int Y=0; Y<NumOutBlocks; ++Y) {
            // W Block
            Memory<DFEVar> WBlock = mem.alloc(Def.DataType, InBlockDim*OutBlockDim);
            WBlock.mapToCPU("W"+ID+"X"+X+"Y"+Y);
            WGridVec[Y][X] <== WBlock.read(WReadAddr);
            // VW Block
            Memory<DFEVar> VWBlock = mem.alloc(Def.DataType, InBlockDim*OutBlockDim);
            VWBlock.mapToCPU("VW"+ID+"X"+X+"Y"+Y);
            VWGridVec[Y][X] <== VWBlock.read(WReadAddr);
        }
    }

    // Bias and VBias Memory Blocks
    DFEVector<DFEVar>  Bias = OutVecType.newInstance(this);
    DFEVector<DFEVar> VBias = OutVecType.newInstance(this);
    for (int Y=0; Y<NumOutBlocks; ++Y) {
        // B Block
        Memory<DFEVar> BBlock = mem.alloc(Def.DataType, OutBlockDim);
        BBlock.mapToCPU("B"+ID+"Y"+Y);
        Bias[Y] <== BBlock.read(ColAddr);
        // VB Block
        Memory<DFEVar> VBBlock = mem.alloc(Def.DataType, OutBlockDim);
        VBBlock.mapToCPU("VB"+ID+"Y"+Y);
        VBias[Y] <== VBBlock.read(ColAddr);
    }


    /////////////////////// Calculation ///////////////////////

    // Accumulator Parameter
    Accumulator.Params Param = Reductions.accumulator.makeAccumulatorConfig(Def.DataType)
                                                     .withEnable(isCalculating)
                                                     .withClear(stream.offset(RowAddr, 1)===0);
    // Calculate Y and Ry
    // TODO: Optimise bit width to avoid overflow
    YVec  = OutVecType.newInstance(this);
    RyVec = OutVecType.newInstance(this);
    for (int Y=0; Y<NumOutBlocks; ++Y) {
        DFEVector<DFEVar> mult_y = PrevLayer * WGridVec[Y];
        DFEVector<DFEVar> mult_Ry;
        if (ID==0) mult_Ry = PrevLayer * VWGridVec[Y];
        else mult_Ry = PrevRyLayer * WGridVec[Y] + PrevLayer * VWGridVec[Y];
        DFEVar toAccumulator_y  = TreeReduce.reduce(new Add<DFEVar>(), mult_y.getElementsAsList());
        DFEVar toAccumulator_Ry = TreeReduce.reduce(new Add<DFEVar>(), mult_Ry.getElementsAsList());
        DFEVar sum_y  = Reductions.accumulator.makeAccumulator(toAccumulator_y,  Param);
        DFEVar sum_Ry = Reductions.accumulator.makeAccumulator(toAccumulator_Ry, Param);
        DFEVar prevActivated_y  = sum_y  +  Bias[Y];
        DFEVar prevActivated_Ry = sum_Ry + VBias[Y];
        switch (AcFunc) {
            case 't': {
                DFEVar Activated_y = tanh(prevActivated_y);
                 YVec[Y] <== Activated_y;
                RyVec[Y] <== prevActivated_Ry * (1 - Activated_y * Activated_y);
                break;
            }
            case 'l': {
                 YVec[Y] <== prevActivated_y;
                RyVec[Y] <== prevActivated_y;
                break;
            }
            default: {
                throw new IllegalArgumentException("Layer["+ID+"]: Activation Function "+AcFunc+" unsupported.");
			}
        }
    }
    
    // For Debug
    debug.simPrintf(Valid, "[%3d] W[%2d][%2d] RdAddr=%2d | Y[0:3][%2d]=(%.12f,%.12f,%.12f,%.12f), Ry[0:3][%2d]=(%.12f,%.12f,%.12f,%.12f), Valid=%d\n", control.count.simpleCounter(32), RowAddr, ColAddr, WReadAddr, ColAddr, YVec[0], YVec[1], YVec[2], YVec[3], ColAddr, RyVec[0], RyVec[1], RyVec[2], RyVec[3], Valid);


	}

    /////////////////////// Activation Functions ///////////////////////

    // tanh() activation function
    // Remarks: temporary workaround: Assign a suitable Data Type to avoid overflow
    // TODO: final solution: use Function Approximation
    private DFEVar tanh(DFEVar x) {
        optimization.pushEnableBitGrowth(true);
        DFEVar exp_2x = KernelMath.exp(2*x);
        DFEVar result = (exp_2x - 1) / (exp_2x + 1);
        optimization.popEnableBitGrowth();
        return result.cast(Def.DataType);
    }


}
